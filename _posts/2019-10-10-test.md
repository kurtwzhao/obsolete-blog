---
title:  "what is this"
date: 2019-10-10
---

## Bellman Euqation

For any fully observed MDP, there exists a policy that is optimal and deterministic. This is not true for partially obverved MDP. It is also not true for games.

$$ V(s)=\max_a\mathbb{E}\{r(s,a)+\gamma V(s')\} $$

The [Euler-Lagrange](https://en.wikipedia.org/wiki/Lagrangian_mechanics) differential equation is the fundamental equation of calculus of variations.

we know that $$\alpha=1$$ is correct

$$
\begin{pmatrix}
\Delta a_i\\\bar{a}\\\Delta\theta_i\\\bar{\theta}
\end{pmatrix}
\sim N\left(
\begin{pmatrix}
0\\\mu_a\\0\\\mu_\theta
\end{pmatrix},
\begin{pmatrix}
\frac{(N-1)(1-\rho_{aa})}N\sigma_a^2 & 0 & \frac{(N-1)(\rho_{a\theta}-\rho_{a\phi})}N\sigma_a\sigma_\theta & 0\\
0 & \frac{(N-1)\rho_{aa}+1}N\sigma_a^2 & 0 & \frac{\rho_{a\theta}+(N-1)\rho_{a\phi}}N\sigma_a\sigma_\theta\\
 \frac{(N-1)(\rho_{a\theta}-\rho_{a\phi})}N\sigma_a\sigma_\theta & 0 & \frac{(N-1)(1-\rho_{\theta\theta})}N\sigma_\theta^2 & 0\\
0 & \frac{\rho_{a\theta}+(N-1)\rho_{a\phi}}N\sigma_a\sigma_\theta & 0 & \frac{(N-1)\rho_{\theta\theta}+1}N\sigma_\theta^2\\
\end{pmatrix}
\right)
$$


This is [an example][1] reference-style link.

Then, anywhere in the document, you define your link label like this, on a line by itself:

[1]: http://example.com/  "Optional hhh Title Here"
