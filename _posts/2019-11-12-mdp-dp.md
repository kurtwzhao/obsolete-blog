---
title: "Markov Decision Process and Dynamic Programming"
date: 2019-11-12
permalink: /2019-11-12-mdp-dp/
---

> Markov decision process is a convenient and formal environment for reinforcement learning and dynamic promming is the only general approach for solving this kind of sequential problems, and even when it is computationally prohibitive, it can serve as the basis for more practical suboptimal approaches.

{: class="table-of-content"}
* TOC
{:toc}



# Markov Decision Process

It is natural to understand MDP by proceeding the definition in the order of **Markov Process**, followed by **Markov Reward Process**, and finally the **Markov Decision Process**. 

## Markov Process

A **Markov process** is defined as a tuple $$(\mathcal{S},\mathbf{P})$$, where $$\mathcal{S}$$ is the state space and $$\mathbf{P}$$ is the transintion probability. We make two additional assumptions that are very common in the reinforcement learning (RL) setting:

* *Finite state space:* we denote the state space by $$\mathcal{S}=\{s_1,s_2,\dots,s_n\}$$, therefore, $$\vert\mathcal{S}\vert<\infty$$
* *Stationary transition probabilities:* the transition probabilities are time independent. Mathematically, this means:

$$P(S_2=s'\vert S_1=s)=P(S_{t+1}=s'\mid S_t=s),\qquad\forall s, s'\in\mathcal{S},\qquad\forall t=1, 2, \dots$$

These two additional assumptions leadto a nice characterization of the transition dynamics in terms of *transition probability matrix* $$\mathbf{P}$$ of size $$\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$$, whose $$(i,j)$$ entry is given by $$\mathbf{P}_{ij}=P(S_{t+1}=s_j\vert S_t=s_i)$$.

## Markov Reward Process

A **Markov reward process** is defined as a tuple $$(\mathcal{S},\mathbf{P},\textcolor{red}{\mathscr{R}},\textcolor{red}{\gamma})$$, where $$\mathscr{R}$$ is a reward funcition that maps states to rewards, i.e. $$\mathscr{R}:\mathcal{S}\to\mathbb{R}$$ and $$\gamma$$ is discount factor between $$0$$ and $$1$$. In Markov reward process, whenever a transition happens from a current state $$s$$ to a successor state $$s'$$, a reward is obtained depending on the current state $$s$$. The reward can be either deterministic or stochastic. If stochastic, tt can be a random variable or even depend on the successor state $$s'$$ and $$R(s)$$ is regraded as the expected reward conditioning on the current state $$s$$. The expectation takes on both the distribution of the random variable and the transition probability on the current state. We assume that the reward is also stationary, i.e. it is also a function of the current state and don't depend on time-step $$t$$.

The return $$G_t$$ is the total discounted reward from time-step $$t$$:

$$G_t\doteq R_{t+1}+\gamma R_{t+2}+\dots=\sum_{k=t+1}^\infty\gamma^{k-t-1}R_k$$

The discount factor $$\gamma\in[0,1)$$ plays two roles: mathematically, it guarantees the return is finite; biologically, humans are impatient or face unvertainty in the future. I want to emphasis here that $$R_t$$ may not be the same as the real realization of the reward if it is stochastic. It is a prior expectation conditioning on the current state. Note that $$G_t$$ is a random variable and its value depends on the realization of the trajectory $$(s_t, s_{t+1},s_{t+2}, \dots)$$. Now we define the value function $$v(s)$$ as the expected return starting from state $$s$$:

$$v(s)\doteq\mathbb{E}[G_t\vert S_t=s]$$

Let $$\mathbf{R}$$ denote a vector of size $$\vert\mathcal{S}\vert\times1$$ whose $$i$$th elemet is $$R(s_i)$$ and $$\mathbf{v}$$ a vector of size $$\vert\mathcal{S}\vert\times1$$ whose $$i$$th elemet is $$v(s_i)$$. Then the value function can be expressed explicitly as:

$$\begin{aligned}
\mathbf{v}&=\mathbf{R}+\gamma\mathbf{PR}+\gamma^2\mathbf{P^2R}+\dots\\&=(\mathbf{I}+\gamma\mathbf{P}+\gamma^2\mathbf{P^2+\dots)\mathbf{R}}\\&=(\mathbf{I}-\gamma\mathbf{P})^{-1}\mathbf{R}
\end{aligned}$$

This last equality comes from the *Neumann series*, it can be regarded as a matrix version of the *geometric series*. Similar to geometric series, it holds when $$\Vert\gamma\mathbf{P}\Vert<1$$, which is true. Since $$\mathbf{P}$$ is a transition matrix, so as $$\mathbf{P^t}$$, and each row of a transitin matrix sums to $$1$$. From the second equlity above one can tell that the summation of each row of the matrix $$(\mathbf{I}-\gamma\mathbf{P})^{-1}$$ equals $$\frac{1}{1-\gamma}$$. Hence the value function can be regraded as a weighted discounted summation of the rewards and the weight is the frequency the state appears.

Another way to derive this result is using the recursive formula. The value function can be expressed recursively:

$$v(s)=R(s)+\gamma\sum_{s'}P(s'\vert s)v(s')$$

if written in the matrix form

$$\mathbf{v}=\mathbf{R}+\gamma\mathbf{Pv}$$

and we obtain the same result. 


## Markov Decision Process
A **Markov decision process** is defined as a tuple $$(\mathcal{S},\textcolor{red}{\mathcal{A}},\mathbf{P},\mathscr{R},\gamma)$$, where $$\mathcal{A}$$ is the action space. Compared with the Markov reward process, there are two main changes with respect to the action $$a$$. First, the reward function depend on not only the current state, but also the action, $$\mathscr{R}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$$. Second, the transition probability is also affected by the action, $$P(S_{t+1}=s'\vert S_t=s,A_t=a)$$. As that in the Markov reward process, the real reward can also depend on the next state $$s'$$, and we still define $$R(s,a)$$ as the expected reward conditioning on the current state $$s$$ and action $$a$$. The popular [Sutton & Barto (2018)](http://incompleteideas.net/book/the-book.html) textbook defines the transition probability as the joint distribution $$P(s',r\vert s,a)$$, since taking expectation is a linear operator, these two kinds of definition are equivalent. The information in the joint distribution of $$(s',r)$$ is unnecessary and only the marginal distribution matters. [I'm curious about that if the return is not defined as the discounted summation of the reward but some other forms, say, the cumulative multiplication, the information in the joint distribution of $$(s',r)$$ can matter. Generally, $$\mathbb{E}[XY]\neq\mathbb{E}[X]\mathbb{E}[Y]$$].

<p style="text-align:center;">
<img src="{{site.url}}/images/RL_illustration.png" width="60%" alt="RL_illustration">
<br>
**Figure 1: illustration of the closed loop control.**
</p>

The element added to Markov decision process from Markov rewrd preocess is the action. Here we distinguish the open loop control and closed loop control and then introduce policy. In open loop control, the control action is independent of the process output or the environment feedback. That means you make a prior plan and take actions according to this plan regardless of how the environment changes. In closed loop control, the action can depend on the feedback of the environment. If the environment is deterministic, there is no difference between open loop control and closed loop control since a previous action determines the later environment deterministically. However, if the environment is stochastic, closed loop control is always better since it takes advantage of current information. A policy is a closed loop control. [If you are familiar with game theory, the strategy there is similar to the policy]. Mathematically, a policy is a function from the state space to a distribution of action space and we denote it by $$\pi(a\vert s)$$. 

If both the state space and the action space are finite, we can represent the transition dynamics by a 3-dimensional tensor $$\mathbf{P}$$ of size $$\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert\times\vert\mathcal{A}\vert$$. Now there is a thrid dimensoion and we call it layer. For each layer of the tensor, it is a tansition matrix with respect to that action.

We can also define the reward and transition matrix for a policy. If it is deterministic, the reward of a policy is just $$\mathbf{R}(\pi)\doteq \mathbf{R}(s,\pi(s))$$ and the transition matrix takes the corresponding layer of the action for the current state and is denoted by a transition matrix $$\mathbf{P}(\pi)$$. [Note that since the 3-D tensor is composed by several layers of tansition matrix, each row still sums to $$1$$. The row of the matrix $$\mathbf{P}(\pi)$$ is taken from some layer of the 3-D tensor corresponding to the policy, hence it is still a transition matrix]. If the policy is stochastic, then the reward and the transiion matrix is defined by:

$$
\begin{aligned}
R_i(\pi)&=\sum_{a\in\mathcal{A}}\pi(a\vert s_i)R(s_i,a)\\
P_{ij}(\pi)&=\sum_{a\in\mathcal{A}}\pi(a\vert s_i)P_{ij}(a)
\end{aligned}
$$
and in the vector/matrix form as $$\mathbf{R}(\pi)$$ and $$\mathbf{P}(\pi)$$.

## Three Tpyes of Markov Decision Process

$$\mathbf{v}=[\mathbf{I}-\gamma\mathbf{P(\pi)}]^{-1}\mathbf{R}(\pi)$$
How to find a best policy?

## Stationary Distribution and Discounted Stationary Distribution

# Dynamic Programming

## Value Iteration


$$V(s)=\max_a\enspace\sum_a\pi(a\vert s)\left[r(s,a)+\gamma\sum_{s'}P(s'\vert s, a)V(s')\right]$$

$$Q(s,a)=\sum_a\pi(a\vert s)\left[r(s,a)+\gamma\sum_{s'}P(s'\vert s, a)\max_{a'}Q(s',a')\right]$$

## Policy Iteration

Although we have the analytic solution for the value function, the computation of the inverse matrix is very expensive, which is $$\mathcal{O}(\vert\mathcal{S}\vert^3)$$. Usually an iterative algorithm is implemented. 

$$V(s)=\enspace\sum_a\pi(a\vert s)\left[r(s,a)+\gamma\sum_{s'}P(s'\vert s, a)V(s')\right]$$

$$Q(s,a)=\sum_a\pi(a\vert s)\left[r(s,a)+\gamma\sum_{s'}P(s'\vert s, a)Q(s',a')\right]$$

## Policy Improvement Theorem



* One policy gradient step does not maximize the objective thoroughly. With improper learning rate, the policy gradient step may be harmful.

## Generalized Policy Iteration
whether this is good

### Value function and action-value function (Q-function)

policy iteration ==> SARSA ==> Actor-Critic
value iteration ==> Q-learning ==> DQN

### value iteration
splitting $$I-\lambda P$$ as $$I-\lambda P = Q_d-R_d$$, then $$v^{n+1}=Q_d^{-1}(R_dv^n+r_d)$$:
* Gauss-Seidel
* Jacobi

over-relaxation and inder-relaxation $$v^{n+1}=v^n+\omega\left[\max_{d\in D}\{Q_d^{-1}(R_dv^n+r_d)\}-v^n\right]$$

policy iteration and modified policy iteration

multistep lookahead

## Value Function Can Decrease in  GPI

# References

[12] Richard S. and Andrew G. Barto. [Reinforcement Learning: An Introduction; 2nd Edition](http://incompleteideas.net/book/the-book.html). 2018.
